{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__\n",
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.R\t connections.py    Models\tresults\t\t www\n",
      "app.sh\t data\t\t   output\tRplots.pdf\n",
      "archive  dominostats.json  __pycache__\tValidation\n",
      "code\t fastapi\t   README.md\tvscode-settings\n"
     ]
    }
   ],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Acc\n",
      "1 AUC\n",
      "2 FP\n",
      "3 FN\n",
      "4 Precision\n",
      "5 Recall\n",
      "6 F1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('dominostats.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "  \n",
    "# Iterating through the json\n",
    "# list\n",
    "for i,k in enumerate(data):\n",
    "    print(i,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Acc': 0.973,\n",
       " 'AUC': 0.979,\n",
       " 'FP': 6.0,\n",
       " 'FN': 33.0,\n",
       " 'Precision': 0.889,\n",
       " 'Recall': 0.593,\n",
       " 'F1': 0.711}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df has 7939 rows and 6 Columns\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle \n",
    "import json\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##Read in data\n",
    "df = pd.read_csv('/mnt/data/smallPrepared.csv', header=0, index_col=0)\n",
    "print(\"df has {} rows and {} Columns\".format(df.shape[0], df.shape[1]))\n",
    "\n",
    "#Split into features and label column\n",
    "columns = list(df.columns)\n",
    "columns.remove('churn_Y')\n",
    "y = df[\"churn_Y\"].values\n",
    "X = df[columns].values\n",
    "\n",
    "\n",
    "#Gradient Boosting\n",
    "gb1 = GradientBoostingClassifier(loss = 'exponential', n_estimators=100)\n",
    "gb1 = gb1.fit(X, y)\n",
    "gb1prb = gb1.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bdd9fe9e2121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmean_fpr_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mprobas_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mpreds_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mfpr_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobas_baseline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_baseline' is not defined"
     ]
    }
   ],
   "source": [
    "# Build and Evaluate New Model\n",
    "loss_ = 'exponential'\n",
    "n_estimators_=100\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(loss = loss_, n_estimators = n_estimators_, learning_rate = 1)\n",
    "clf = clf.fit(X, y)\n",
    "clfprb = clf.predict_proba(X)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "kf = KFold(n_splits=5)\n",
    " \n",
    "tprs = []\n",
    "aucs = []\n",
    "accs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    " \n",
    "i = 0\n",
    "for train, test in kf.split(X):\n",
    "    probas_ = clf.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    preds_ = clf.fit(X[train], y[train]).predict(X[test])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    accuracy = metrics.accuracy_score(y[test], preds_)\n",
    "    accs.append(accuracy)\n",
    "    i += 1\n",
    "    \n",
    "mean_acc = np.mean(accs)\n",
    "std_acc = np.std(accs)\n",
    "mean_tpr = np.mean(tprs, axis = 0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    " \n",
    "#Get Baseline Model\n",
    " \n",
    "baseline_model = pickle.load(open('/mnt/results/baseline.pkl', 'rb'))\n",
    "tprs_baseline = []\n",
    "aucs_baseline = []\n",
    "accs_baseline = []\n",
    "mean_fpr_baseline = np.linspace(0, 1, 100)\n",
    " \n",
    "probas_baseline = baseline_model.predict_proba(X_baseline)\n",
    "preds_baseline = baseline_model.predict(X_baseline)\n",
    "fpr_baseline, tpr_baseline, thresholds_baseline = metrics.roc_curve(y_baseline, probas_baseline[:, 1])\n",
    "tprs_baseline.append(np.interp(mean_fpr_baseline, fpr_baseline, tpr_baseline))\n",
    "tprs[-1][0] = 0.0\n",
    "roc_auc_baseline = metrics.auc(fpr_baseline, tpr_baseline)\n",
    "aucs_baseline.append(roc_auc_baseline)\n",
    "accuracy_baseline = metrics.accuracy_score(y_baseline, preds_baseline)\n",
    "accs_baseline.append(accuracy_baseline)\n",
    "acc_baseline = np.mean(accs_baseline)\n",
    "auc_baseline = np.mean(aucs_baseline)\n",
    " \n",
    "# Make ROC Plot\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.8)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b', lw=2, alpha=.8, label='New: Avg AUC = %0.3f Avg Acc = %0.3f' % (mean_auc, mean_acc))\n",
    "plt.plot(fpr_baseline,tpr_baseline,color='red', label=        'Baseline: AUC = %0.3f Acc = %0.3f' % (auc_baseline, acc_baseline))\n",
    "#plt.text(0.6, 0.125, 'Mean AUC = %0.3f' % (mean_auc))\n",
    "#plt.text(0.6, 0.2, 'Mean Accuracy = %0.3f' % (mean_acc))\n",
    " \n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + 2*std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - 2*std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 2 std. dev.')\n",
    " \n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"5-Fold CV ROC Curve and Metrics: Loss = \" + loss_ + \", N_Estimators = \" + str(n_estimators_))\n",
    "plt.legend(loc=\"lower right\")\n",
    " \n",
    "#write out resutls\n",
    "plt.savefig('/mnt/results/DeployReadyModelAUC_ACC'+'.png', format='png')\n",
    "plt.gcf().clear()\n",
    " \n",
    "#make confusion matrix plot\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    " \n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    " \n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    " \n",
    "    title:        the text to display at the top of the matrix\n",
    " \n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    " \n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    " \n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    " \n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    " \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    " \n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    " \n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    " \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    " \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    " \n",
    " \n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    " \n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.gcf().subplots_adjust(bottom=0.25)\n",
    "    plt.savefig('/mnt/results/DeployReadyConfMatx.png', format='png')\n",
    "    plt.gcf().clear()\n",
    "    \n",
    "plot_confusion_matrix(cm           = metrics.confusion_matrix(y[test], preds_), \n",
    "                      normalize    = False,\n",
    "                      target_names = ['no churn', 'churn'],\n",
    "                      title        = \"Confusion Matrix for Loss = \" + loss_ + \", N_Estimators = \" + str(n_estimators_))\n",
    "                      \n",
    "import json\n",
    " \n",
    "with open('dominostats.json', 'w') as f:\n",
    "    f.write(json.dumps({\"Acc\": round(mean_acc, 3), \"AUC\": round(mean_auc, 3)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
