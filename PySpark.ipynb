{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import random\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master',\n",
       "  'spark://spark-60a43dd685736767aa903d9d-master-svc.domino-vip2-compute.svc.cluster.local:7077'),\n",
       " ('spark.driver.port', '33081'),\n",
       " ('spark.driver.host',\n",
       "  'spark-driver-60a43dd685736767aa903d9d.domino-vip2-compute.svc.cluster.local'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.executor.memory', '4096m'),\n",
       " ('spark.ui.reverseProxyUrl',\n",
       "  '/elliott_botwick/MLOps-Demo/workspaces/60a43dd685736767aa903d9d/spark/ui/'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.cores', '1'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'app-20210518231909-0001'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.ui.reverseProxy', 'true'),\n",
       " ('spark.ui.proxyBase', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-driver-60a43dd685736767aa903d9d.domino-vip2-compute.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-60a43dd685736767aa903d9d-master-svc.domino-vip2-compute.svc.cluster.local:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-60a43dd685736767aa903d9d-master-svc.domino-vip2-compute.svc.cluster.local:7077 appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.conf import SparkConf\n",
    "# SparkSession.builder.config(conf=SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.139664\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 1000000\n",
    " \n",
    "def inside(p):\n",
    " x, y = random.random(), random.random()\n",
    " return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "pi = float(4.0 * count / float(NUM_SAMPLES))\n",
    "print('Pi is roughly', pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline.csv  churndata.csv  prepared.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /domino/datasets/local/testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(infer_schema = True, header = True).load(('../..//domino/datasets/local/testData/prepared.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----+-------------------+------------+----+------+------------------+------------------+---+------+-------+-----+---------+---------+--------+----+-------+\n",
      "|custid|            dropperc|mins|      KPI_DeviceSat|consecmonths|long|income|   KPI_NetworkQual|          dataperc|age|  bill|datause|calls|education|servcalls|dataplan|kids|churn_Y|\n",
      "+------+--------------------+----+-------------------+------------+----+------+------------------+------------------+---+------+-------+-----+---------+---------+--------+----+-------+\n",
      "|844336|  0.0163636363636364| 550| 0.0160345039664219|          28|  35|  89.2|0.7603546409999999|    0.158935546875| 45| 71.44|    651|   55|       11|        0|    4096|   2|      0|\n",
      "|146041|0.018348623853211003| 545| 0.0170551148054854|          33|  35|  54.2|       0.526147204|      0.3779296875| 43|124.23|    774|   66|        9|        0|    2048|   1|      0|\n",
      "|847745|  0.0185185185185185| 378|0.00889616180366376|          41|  19|  55.3|       0.798810583|      0.2822265625| 41|112.42|    578|   91|       12|        0|    2048|   1|      0|\n",
      "|285565|  0.0144927536231884| 552| 0.0164598085384115|          32|  38|  66.8|       0.631074633|    0.271240234375| 31|105.12|   1111|   71|       10|        0|    4096|   0|      0|\n",
      "|754611|  0.0121317157712305| 577| 0.0207106576742021|           4|  31|  87.2|        0.72642769|     0.19873046875| 43|109.48|    407|   56|        8|        0|    2048|   0|      0|\n",
      "|100018|  0.0244360902255639| 532| 0.0110610845340877|          26|  30|  57.6|0.7603546409999999|0.0769856770833333| 37| 126.5|    473|   76|        8|        0|    6144|   1|      0|\n",
      "|195381|  0.0139664804469274| 716| 0.0141284022811508|          18|  39|  78.2|       0.586984301|      0.1298828125| 48|105.88|    798|   60|        6|        0|    6144|   1|      0|\n",
      "|665504|  0.0139720558882236| 501| 0.0173072488274286|          22|  33|  79.2|0.7464753270000001|    0.351318359375| 57|119.27|   1439|   64|       11|        0|    4096|   2|      0|\n",
      "|529231|  0.0238095238095238| 462| 0.0171973594318547|          13|  27| 102.3|       0.480935685|     0.12841796875| 49| 88.18|    789|   51|        8|        0|    6144|   2|      0|\n",
      "|358025|   0.022883295194508| 437| 0.0164883365616948|          42|  26|  81.6|       0.637437199|               0.0| 59| 73.33|      0|   49|        9|        0|    6144|   1|      0|\n",
      "+------+--------------------+----+-------------------+------------+----+------+------------------+------------------+---+------+-------+-----+---------+---------+--------+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7939"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_access_key = os.environ['AWS_ACCESS_KEY_ID']\n",
    "s3_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "s3 = boto3.resource(\"s3\", aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o242.csv.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f0057f7d6ce6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3://seteam-files/mq/2016.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o242.csv.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.read.csv('s3://seteam-files/mq/2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine = spark.read.csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.ui.proxyBase', '.'),\n",
       " ('spark.driver.port', '39741'),\n",
       " ('spark.driver.host',\n",
       "  'spark-driver-60245d4a0ac6130d45aab9ea.domino-vip2-compute.svc.cluster.local'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'app-20210210222631-0000'),\n",
       " ('spark.master',\n",
       "  'spark://spark-60245d4a0ac6130d45aab9ea-master-svc.domino-vip2-compute.svc.cluster.local:7077'),\n",
       " ('spark.executor.memory', '4096m'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.instances', '1'),\n",
       " ('spark.app.name', 'MyAppName'),\n",
       " ('spark.executor.cores', '1'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.ui.reverseProxy', 'true'),\n",
       " ('spark.ui.reverseProxyUrl',\n",
       "  '/elliott_botwick/SparkWineQuality/workspaces/60245d4a0ac6130d45aab9ea/spark/ui/')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup Pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"MyAppName\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# You can examine the full config\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-driver-60245d4a0ac6130d45aab9ea.domino-vip2-compute.svc.cluster.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-60245d4a0ac6130d45aab9ea-master-svc.domino-vip2-compute.svc.cluster.local:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-60245d4a0ac6130d45aab9ea-master-svc.domino-vip2-compute.svc.cluster.local:7077 appName=MyAppName>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup and Test Spark Context\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Profile Data\n",
    "red_wine.printSchema()\n",
    "print(\"Rows: %s\" % red_wine.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: double (nullable = true)\n",
      " |-- volatile acidity: double (nullable = true)\n",
      " |-- citric acid: double (nullable = true)\n",
      " |-- residual sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free sulfur dioxide: double (nullable = true)\n",
      " |-- total sulfur dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feature Extraction into Vectors\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# select the columns to be used as the features (all except `quality`)\n",
    "featureColumns = [c for c in red_wine.columns if c != 'quality']\n",
    "\n",
    "# create and configure the assembler\n",
    "assembler = VectorAssembler(inputCols=featureColumns, \n",
    "                            outputCol=\"features\")\n",
    "\n",
    "# transform the original data\n",
    "dataDF = assembler.transform(red_wine)\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+--------------------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|            features|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+--------------------+\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|[7.4,0.7,0.0,1.9,...|\n",
      "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|[7.8,0.88,0.0,2.6...|\n",
      "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|[7.8,0.76,0.04,2....|\n",
      "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|      6|[11.2,0.28,0.56,1...|\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|[7.4,0.7,0.0,1.9,...|\n",
      "|          7.4|            0.66|        0.0|           1.8|    0.075|               13.0|                40.0| 0.9978|3.51|     0.56|    9.4|      5|[7.4,0.66,0.0,1.8...|\n",
      "|          7.9|             0.6|       0.06|           1.6|    0.069|               15.0|                59.0| 0.9964| 3.3|     0.46|    9.4|      5|[7.9,0.6,0.06,1.6...|\n",
      "|          7.3|            0.65|        0.0|           1.2|    0.065|               15.0|                21.0| 0.9946|3.39|     0.47|   10.0|      7|[7.3,0.65,0.0,1.2...|\n",
      "|          7.8|            0.58|       0.02|           2.0|    0.073|                9.0|                18.0| 0.9968|3.36|     0.57|    9.5|      7|[7.8,0.58,0.02,2....|\n",
      "|          7.5|             0.5|       0.36|           6.1|    0.071|               17.0|               102.0| 0.9978|3.35|      0.8|   10.5|      5|[7.5,0.5,0.36,6.1...|\n",
      "|          6.7|            0.58|       0.08|           1.8|    0.097|               15.0|                65.0| 0.9959|3.28|     0.54|    9.2|      5|[6.7,0.58,0.08,1....|\n",
      "|          7.5|             0.5|       0.36|           6.1|    0.071|               17.0|               102.0| 0.9978|3.35|      0.8|   10.5|      5|[7.5,0.5,0.36,6.1...|\n",
      "|          5.6|           0.615|        0.0|           1.6|    0.089|               16.0|                59.0| 0.9943|3.58|     0.52|    9.9|      5|[5.6,0.615,0.0,1....|\n",
      "|          7.8|            0.61|       0.29|           1.6|    0.114|                9.0|                29.0| 0.9974|3.26|     1.56|    9.1|      5|[7.8,0.61,0.29,1....|\n",
      "|          8.9|            0.62|       0.18|           3.8|    0.176|               52.0|               145.0| 0.9986|3.16|     0.88|    9.2|      5|[8.9,0.62,0.18,3....|\n",
      "|          8.9|            0.62|       0.19|           3.9|     0.17|               51.0|               148.0| 0.9986|3.17|     0.93|    9.2|      5|[8.9,0.62,0.19,3....|\n",
      "|          8.5|            0.28|       0.56|           1.8|    0.092|               35.0|               103.0| 0.9969| 3.3|     0.75|   10.5|      7|[8.5,0.28,0.56,1....|\n",
      "|          8.1|            0.56|       0.28|           1.7|    0.368|               16.0|                56.0| 0.9968|3.11|     1.28|    9.3|      5|[8.1,0.56,0.28,1....|\n",
      "|          7.4|            0.59|       0.08|           4.4|    0.086|                6.0|                29.0| 0.9974|3.38|      0.5|    9.0|      4|[7.4,0.59,0.08,4....|\n",
      "|          7.9|            0.32|       0.51|           1.8|    0.341|               17.0|                56.0| 0.9969|3.04|     1.08|    9.2|      6|[7.9,0.32,0.51,1....|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# fit a `LinearRegression` model using features in colum `features` and label in column `quality`\n",
    "lr = LinearRegression(maxIter=30, regParam=0.3, elasticNetParam=0.3, featuresCol=\"features\", labelCol=\"quality\")\n",
    "lrModel = lr.fit(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fixed acidity', 0.0)\n",
      "('volatile acidity', -0.7569245708156528)\n",
      "('citric acid', 0.0)\n",
      "('residual sugar', 0.0)\n",
      "('chlorides', 0.0)\n",
      "('free sulfur dioxide', 0.0)\n",
      "('total sulfur dioxide', -0.00020326904604336523)\n",
      "('density', 0.0)\n",
      "('pH', 0.0)\n",
      "('sulphates', 0.2738470451309209)\n",
      "('alcohol', 0.19463722248782067)\n"
     ]
    }
   ],
   "source": [
    "for t in zip(featureColumns, lrModel.coefficients):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+--------------------+------------------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|            features|        prediction|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+--------------------+------------------+\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|[7.4,0.7,0.0,1.9,...|5.2822415832023255|\n",
      "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|[7.8,0.88,0.0,2.6...| 5.250003816346916|\n",
      "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|[7.8,0.76,0.04,2....|  5.33526185108943|\n",
      "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|      6|[11.2,0.28,0.56,1...| 5.678196737645519|\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|[7.4,0.7,0.0,1.9,...|5.2822415832023255|\n",
      "|          7.4|            0.66|        0.0|           1.8|    0.075|               13.0|                40.0| 0.9978|3.51|     0.56|    9.4|      5|[7.4,0.66,0.0,1.8...| 5.311298951758691|\n",
      "|          7.9|             0.6|       0.06|           1.6|    0.069|               15.0|                59.0| 0.9964| 3.3|     0.46|    9.4|      5|[7.9,0.6,0.06,1.6...| 5.325467609619714|\n",
      "|          7.3|            0.65|        0.0|           1.2|    0.065|               15.0|                21.0| 0.9946|3.39|     0.47|   10.0|      7|[7.3,0.65,0.0,1.2...| 5.414866408772581|\n",
      "|          7.8|            0.58|       0.02|           2.0|    0.073|                9.0|                18.0| 0.9968|3.36|     0.57|    9.5|      7|[7.8,0.58,0.02,2....| 5.398527029136989|\n",
      "|          7.5|             0.5|       0.36|           6.1|    0.071|               17.0|               102.0| 0.9978|3.35|      0.8|   10.5|      5|[7.5,0.5,0.36,6.1...| 5.699628437802531|\n",
      "|          6.7|            0.58|       0.08|           1.8|    0.097|               15.0|                65.0| 0.9959|3.28|     0.54|    9.2|      5|[6.7,0.58,0.08,1....| 5.322366805872677|\n",
      "|          7.5|             0.5|       0.36|           6.1|    0.071|               17.0|               102.0| 0.9978|3.35|      0.8|   10.5|      5|[7.5,0.5,0.36,6.1...| 5.699628437802531|\n",
      "|          5.6|           0.615|        0.0|           1.6|    0.089|               16.0|                59.0| 0.9943|3.58|     0.52|    9.9|      5|[5.6,0.615,0.0,1....| 5.427863175009246|\n",
      "|          7.8|            0.61|       0.29|           1.6|    0.114|                9.0|                29.0| 0.9974|3.26|     1.56|    9.1|      5|[7.8,0.61,0.29,1....| 5.566837018190526|\n",
      "|          8.9|            0.62|       0.18|           3.8|    0.176|               52.0|               145.0| 0.9986|3.16|     0.88|    9.2|      5|[8.9,0.62,0.18,3....| 5.368936294701094|\n",
      "|          8.9|            0.62|       0.19|           3.9|     0.17|               51.0|               148.0| 0.9986|3.17|     0.93|    9.2|      5|[8.9,0.62,0.19,3....|  5.38201883981951|\n",
      "|          8.5|            0.28|       0.56|           1.8|    0.092|               35.0|               103.0| 0.9969| 3.3|     0.75|   10.5|      7|[8.5,0.28,0.56,1....| 5.852256222079385|\n",
      "|          8.1|            0.56|       0.28|           1.7|    0.368|               16.0|                56.0| 0.9968|3.11|     1.28|    9.3|      5|[8.1,0.56,0.28,1....| 5.561445254349044|\n",
      "|          7.4|            0.59|       0.08|           4.4|    0.086|                6.0|                29.0| 0.9974|3.38|      0.5|    9.0|      4|[7.4,0.59,0.08,4....|  5.27223391951928|\n",
      "|          7.9|            0.32|       0.51|           1.8|    0.341|               17.0|                56.0| 0.9969|3.04|     1.08|    9.2|      6|[7.9,0.32,0.51,1....| 5.668874020069834|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict the quality, the predicted quality will be saved in `prediction` column\n",
    "predictionsDF = lrModel.transform(dataDF)\n",
    "display(predictionsDF.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) = 0.683189\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# create a regression evaluator with RMSE metrics\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='quality', predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictionsDF)\n",
    "print(\"Root Mean Squared Error (RMSE) = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the input data into traning and test dataframes with 70% to 30% weights\n",
    "(trainingDF, testDF) = red_wine.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training data = 0.678189\n",
      "RMSE on test data = 0.701203\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# construct the `Pipeline` that with two stages: the `vector assembler` and `regresion model estimator`\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# train the pipleline on the traning data\n",
    "lrPipelineModel = pipeline.fit(trainingDF)\n",
    "\n",
    "# make predictions\n",
    "traningPredictionsDF = lrPipelineModel.transform(trainingDF)\n",
    "testPredictionsDF = lrPipelineModel.transform(testDF)\n",
    "\n",
    "# evaluate the model on test and traning data\n",
    "print(\"RMSE on training data = %g\" % evaluator.evaluate(traningPredictionsDF))\n",
    "print(\"RMSE on test data = %g\" % evaluator.evaluate(testPredictionsDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Random Forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# define the random forest estimator\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"quality\", numTrees=100, maxBins=128, maxDepth=20, \\\n",
    "                           minInstancesPerNode=5, seed=33)\n",
    "rfPipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# train the random forest model\n",
    "rfPipelineModel = rfPipeline.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE on training data = 0.381986\n",
      "Random Forest RMSE on test data = 0.601508\n"
     ]
    }
   ],
   "source": [
    "#Test Accuracy of Random Forest\n",
    "rfTrainingPredictions = rfPipelineModel.transform(trainingDF)\n",
    "rfTestPredictions = rfPipelineModel.transform(testDF)\n",
    "print(\"Random Forest RMSE on training data = %g\" % evaluator.evaluate(rfTrainingPredictions))\n",
    "print(\"Random Forest RMSE on test data = %g\" % evaluator.evaluate(rfTestPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
